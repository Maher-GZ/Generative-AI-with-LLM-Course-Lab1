{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install some packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\dell\\anaconda3\\lib\\site-packages (24.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement torch==1.13.1 (from versions: 2.0.0, 2.0.1, 2.1.0, 2.1.1, 2.1.2, 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1)\n",
      "ERROR: No matching distribution found for torch==1.13.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip \n",
    "%pip install --disable-pip-version-check torch==1.13.1 torchdata==0.5.1 --quiet\n",
    "%pip install transformers==4.27.2 datasets==2.11.0 --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import some libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/DELL/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-cd36827d3490488d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "519d7ecf297c4c94868121b6243036ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "huggingface_dataset_name= \"knkarthick/dialogsum\"\n",
    "dataset= load_dataset(huggingface_dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore this dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___________________________________________________________________________________________________\n",
      "Example  1\n",
      "___________________________________________________________________________________________________\n",
      "INPUT DIALOGUE: \n",
      "#Person1#: Ms. Dawson, I need you to take a dictation for me.\n",
      "#Person2#: Yes, sir...\n",
      "#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\n",
      "#Person2#: Yes, sir. Go ahead.\n",
      "#Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited.\n",
      "#Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications?\n",
      "#Person1#: It should apply to all communications, not only in this office between employees, but also any outside communications.\n",
      "#Person2#: But sir, many employees use Instant Messaging to communicate with their clients.\n",
      "#Person1#: They will just have to change their communication methods. I don't want any - one using Instant Messaging in this office. It wastes too much time! Now, please continue with the memo. Where were we?\n",
      "#Person2#: This applies to internal and external communications.\n",
      "#Person1#: Yes. Any employee who persists in using Instant Messaging will first receive a warning and be placed on probation. At second offense, the employee will face termination. Any questions regarding this new policy may be directed to department heads.\n",
      "#Person2#: Is that all?\n",
      "#Person1#: Yes. Please get this memo typed up and distributed to all employees before 4 pm.\n",
      "___________________________________________________________________________________________________\n",
      "BASELINE HUMAN SUMMARY:\n",
      "In order to prevent employees from wasting time on Instant Message programs, #Person1# decides to terminate the use of those programs and asks Ms. Dawson to send out a memo to all employees by the afternoon.\n",
      "___________________________________________________________________________________________________\n",
      "___________________________________________________________________________________________________\n",
      "Example  2\n",
      "___________________________________________________________________________________________________\n",
      "INPUT DIALOGUE: \n",
      "#Person1#: Is anybody in?\n",
      "#Person2#: How can I help you?\n",
      "#Person1#: I have a headache.\n",
      "#Person2#: Let me take your temperature with a thermometer.\n",
      "#Person1#: OK.\n",
      "#Person2#: I think you have a small fever.\n",
      "#Person1#: I thought so. I felt dizzy this morning.\n",
      "#Person2#: You should've called in sick! Next time, have either of your parents call the school office.\n",
      "___________________________________________________________________________________________________\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person2# finds that #Person1# has a fever and says #Person1# should've called in sick earlier.\n",
      "___________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "example_indices=[1,55]\n",
    "dash_line ='_'.join(\"\" for x in range(100))\n",
    "\n",
    "for i , index in enumerate(example_indices):\n",
    "    print(dash_line)\n",
    "    print('Example ', i+1)\n",
    "    print(dash_line)\n",
    "    print(\"INPUT DIALOGUE: \")\n",
    "    print(dataset['test'] [ index] ['dialogue'])\n",
    "    print(dash_line)\n",
    "    print(\"BASELINE HUMAN SUMMARY:\")\n",
    "    print(dataset['test'][index]['summary'])\n",
    "    print(dash_line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the pretrained model & Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Here are some pretrained models that we can use and there cons\n",
    "\n",
    "BART (Bidirectional and Auto-Regressive Transformers):\n",
    "\n",
    "Model: facebook/bart-large-cnn\n",
    "Pros: Strong performance on various summarization benchmarks, handles long sequences well, pre-trained on diverse datasets.\n",
    "Use Case: General-purpose summarization, including dialogues.\n",
    "T5 (Text-To-Text Transfer Transformer):\n",
    "\n",
    "Model: t5-large or t5-base\n",
    "Pros: Flexible text-to-text framework, strong performance across multiple NLP tasks including summarization.\n",
    "Use Case: Summarization, translation, question answering, and other text generation tasks.\n",
    "Pegasus (Pre-training with Extracted Gap-sentences for Abstractive Summarization):\n",
    "\n",
    "Model: google/pegasus-large\n",
    "Pros: Designed specifically for summarization, excellent performance on summarization tasks, trained on a large corpus.\n",
    "Use Case: High-quality abstractive summarization.\n",
    "\n",
    "we're gonna use flan-t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name= \"google/flan-t5-base\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=AutoTokenizer.from_pretrained (model_name,use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize & Embading ====> Encoding &  vice versa ====> Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___________________________________________________________________________________________________\n",
      "Example 1\n",
      "___________________________________________________________________________________________________\n",
      "INPUT PROMPT:\n",
      "#Person1#: Ms. Dawson, I need you to take a dictation for me.\n",
      "#Person2#: Yes, sir...\n",
      "#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\n",
      "#Person2#: Yes, sir. Go ahead.\n",
      "#Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited.\n",
      "#Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications?\n",
      "#Person1#: It should apply to all communications, not only in this office between employees, but also any outside communications.\n",
      "#Person2#: But sir, many employees use Instant Messaging to communicate with their clients.\n",
      "#Person1#: They will just have to change their communication methods. I don't want any - one using Instant Messaging in this office. It wastes too much time! Now, please continue with the memo. Where were we?\n",
      "#Person2#: This applies to internal and external communications.\n",
      "#Person1#: Yes. Any employee who persists in using Instant Messaging will first receive a warning and be placed on probation. At second offense, the employee will face termination. Any questions regarding this new policy may be directed to department heads.\n",
      "#Person2#: Is that all?\n",
      "#Person1#: Yes. Please get this memo typed up and distributed to all employees before 4 pm.\n",
      "___________________________________________________________________________________________________\n",
      "BASELINE HUMAN SUMMARY: \n",
      " In order to prevent employees from wasting time on Instant Message programs, #Person1# decides to terminate the use of those programs and asks Ms. Dawson to send out a memo to all employees by the afternoon.\n",
      "___________________________________________________________________________________________________\n",
      "MODEL GENERATION WITHOUT PROMPT ENGINEERING:\n",
      "#Person1#: Ms. Dawson, I need you to take a dictation for me.\n",
      "___________________________________________________________________________________________________\n",
      "Example 2\n",
      "___________________________________________________________________________________________________\n",
      "INPUT PROMPT:\n",
      "#Person1#: I want to give our kitchen the works. \n",
      "#Person2#: Why? I think it's convenient and good. \n",
      "#Person1#: No. The decoration has been out-of-date. My friends have a whole kitchen now. It's modern. \n",
      "#Person2#: Why should we run after the fashion? \n",
      "#Person1#: I am not running after the fashion. I just want a very beautiful and clean kitchen. \n",
      "___________________________________________________________________________________________________\n",
      "BASELINE HUMAN SUMMARY: \n",
      " #Person1# wants to refurbish the kitchen while #Person2# thinks it unnecessary.\n",
      "___________________________________________________________________________________________________\n",
      "MODEL GENERATION WITHOUT PROMPT ENGINEERING:\n",
      "Person1#: I think it's convenient and good.\n"
     ]
    }
   ],
   "source": [
    "for i, index in enumerate( example_indices):\n",
    "    dialogue = dataset['test'][index]['dialogue']\n",
    "    summary = dataset['test'][index]['summary']\n",
    "\n",
    "    inputs= tokenizer(dialogue,return_tensors='pt') # it returns a pytorch tensor\n",
    "    output= tokenizer.decode(model.generate(\n",
    "        inputs['input_ids'], # input_ids ==> is a part from the input tensor consist from token vectors \n",
    "        max_new_tokens=50\n",
    "\n",
    "    )[0],\n",
    "    skip_special_tokens=True)\n",
    "    print(dash_line)\n",
    "    print(\"Example\",i+1)\n",
    "    print(dash_line)\n",
    "    print(f'INPUT PROMPT:\\n{dialogue}')\n",
    "    print(dash_line)\n",
    "    print(f'BASELINE HUMAN SUMMARY: \\n {summary}')\n",
    "    print(dash_line)\n",
    "    print(f'MODEL GENERATION WITHOUT PROMPT ENGINEERING:\\n{output}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IN-CONTEXT LEARNING "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero shot With prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___________________________________________________________________________________________________\n",
      "Example 1\n",
      "___________________________________________________________________________________________________\n",
      "INPUT PROMPT:\n",
      "#Person1#: Ms. Dawson, I need you to take a dictation for me.\n",
      "#Person2#: Yes, sir...\n",
      "#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\n",
      "#Person2#: Yes, sir. Go ahead.\n",
      "#Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited.\n",
      "#Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications?\n",
      "#Person1#: It should apply to all communications, not only in this office between employees, but also any outside communications.\n",
      "#Person2#: But sir, many employees use Instant Messaging to communicate with their clients.\n",
      "#Person1#: They will just have to change their communication methods. I don't want any - one using Instant Messaging in this office. It wastes too much time! Now, please continue with the memo. Where were we?\n",
      "#Person2#: This applies to internal and external communications.\n",
      "#Person1#: Yes. Any employee who persists in using Instant Messaging will first receive a warning and be placed on probation. At second offense, the employee will face termination. Any questions regarding this new policy may be directed to department heads.\n",
      "#Person2#: Is that all?\n",
      "#Person1#: Yes. Please get this memo typed up and distributed to all employees before 4 pm.\n",
      "___________________________________________________________________________________________________\n",
      "BASELINE HUMAN SUMMARY: \n",
      " In order to prevent employees from wasting time on Instant Message programs, #Person1# decides to terminate the use of those programs and asks Ms. Dawson to send out a memo to all employees by the afternoon.\n",
      "___________________________________________________________________________________________________\n",
      "MODEL GENERATION WITH ZERO PROMPTED SHOT :\n",
      "The memo is to be distributed to all employees by this afternoon.\n",
      "___________________________________________________________________________________________________\n",
      "Example 2\n",
      "___________________________________________________________________________________________________\n",
      "INPUT PROMPT:\n",
      "#Person1#: I want to give our kitchen the works. \n",
      "#Person2#: Why? I think it's convenient and good. \n",
      "#Person1#: No. The decoration has been out-of-date. My friends have a whole kitchen now. It's modern. \n",
      "#Person2#: Why should we run after the fashion? \n",
      "#Person1#: I am not running after the fashion. I just want a very beautiful and clean kitchen. \n",
      "___________________________________________________________________________________________________\n",
      "BASELINE HUMAN SUMMARY: \n",
      " #Person1# wants to refurbish the kitchen while #Person2# thinks it unnecessary.\n",
      "___________________________________________________________________________________________________\n",
      "MODEL GENERATION WITH ZERO PROMPTED SHOT :\n",
      "The person wants to give their kitchen the works.\n"
     ]
    }
   ],
   "source": [
    "for i, index in enumerate( example_indices):\n",
    "    dialogue = dataset['test'][index]['dialogue']\n",
    "    summary = dataset['test'][index]['summary']\n",
    "\n",
    "    prompt= f\"\"\"Dialogue\n",
    "\n",
    "    {dialogue}\n",
    "    \n",
    "    Summary: \n",
    "    \"\"\"\n",
    "\n",
    "    inputs= tokenizer(prompt,return_tensors='pt') # it returns a pytorch tensor\n",
    "    output= tokenizer.decode(model.generate(\n",
    "        inputs['input_ids'], # input_ids ==> is a part from the input tensor consist from token vectors \n",
    "        max_new_tokens=50\n",
    "\n",
    "    )[0],\n",
    "    skip_special_tokens=True)\n",
    "    print(dash_line)\n",
    "    print(\"Example\",i+1)\n",
    "    print(dash_line)\n",
    "    print(f'INPUT PROMPT:\\n{dialogue}')\n",
    "    print(dash_line)\n",
    "    print(f'BASELINE HUMAN SUMMARY: \\n {summary}')\n",
    "    print(dash_line)\n",
    "    print(f'MODEL GENERATION WITH ZERO PROMPTED SHOT :\\n{output}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt (example_indices,example_indices_to_summarize ):\n",
    "    prompt = \"\"\n",
    "    if isinstance(example_indices, ( float, str)):\n",
    "        print( \"not accepted ==> must me integer of list of integers\") \n",
    "    else:\n",
    "        if  isinstance(example_indices, (int)):\n",
    "            example_indices= [example_indices]\n",
    "        else :\n",
    "            example_indices=example_indices\n",
    "        for j in example_indices:\n",
    "            dialogue= dataset['test'][j]['dialogue']\n",
    "            summary = dataset['test'][j]['summary']\n",
    "            prompt += f\"\"\"Dialouge: \n",
    "\n",
    "            {dialogue}\n",
    "\n",
    "            summary: \n",
    "\n",
    "            {summary}\n",
    "\n",
    "            \"\"\"   \n",
    "        prompt +=f\"\"\"Dialogue: \n",
    "\n",
    "        {dataset['test'][example_indices_to_summarize]['dialogue']}\n",
    "\n",
    "        Summary: \n",
    "        \n",
    "        \"\"\"\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Shot - ONE COMPLETE EXAMPLE - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___________________________________________________________________________________________________\n",
      "Example 1\n",
      "___________________________________________________________________________________________________\n",
      "BASELINE HUMAN SUMMARY: \n",
      " In order to prevent employees from wasting time on Instant Message programs, #Person1# decides to terminate the use of those programs and asks Ms. Dawson to send out a memo to all employees by the afternoon.\n",
      "___________________________________________________________________________________________________\n",
      "MODEL GENERATION WITH ZERO SHOT :\n",
      "The memo is to be distributed to all employees by this afternoon.\n",
      "___________________________________________________________________________________________________\n",
      "Example 2\n",
      "___________________________________________________________________________________________________\n",
      "BASELINE HUMAN SUMMARY: \n",
      " #Person1# wants to refurbish the kitchen while #Person2# thinks it unnecessary.\n",
      "___________________________________________________________________________________________________\n",
      "MODEL GENERATION WITH ZERO SHOT :\n",
      "Person1 wants to give their kitchen the works.\n"
     ]
    }
   ],
   "source": [
    "for i, index in enumerate( example_indices):\n",
    "    summary = dataset['test'][index]['summary']\n",
    "\n",
    "    prompt= make_prompt(34,index)\n",
    "\n",
    "\n",
    "\n",
    "    inputs= tokenizer(prompt,return_tensors='pt') # it returns a pytorch tensor\n",
    "    output= tokenizer.decode(model.generate(\n",
    "        inputs['input_ids'], # input_ids ==> is a part from the input tensor consist from token vectors \n",
    "        max_new_tokens=50\n",
    "\n",
    "    )[0],\n",
    "    skip_special_tokens=True)\n",
    "    print(dash_line)\n",
    "    print(\"Example\",i+1)\n",
    "    print(dash_line)\n",
    "    print(f\"BASELINE HUMAN SUMMARY: \\n {summary}\")\n",
    "    print(dash_line)\n",
    "    print(f'MODEL GENERATION WITH ZERO SHOT :\\n{output}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few Shots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___________________________________________________________________________________________________\n",
      "Example 1\n",
      "___________________________________________________________________________________________________\n",
      "BASELINE HUMAN SUMMARY: \n",
      " In order to prevent employees from wasting time on Instant Message programs, #Person1# decides to terminate the use of those programs and asks Ms. Dawson to send out a memo to all employees by the afternoon.\n",
      "___________________________________________________________________________________________________\n",
      "MODEL GENERATION WITH FEW SHOTS SHOT :\n",
      "Ms. Dawson takes a dictation for #Person1# about prohibiting the use of Instant Message programs in the office.\n",
      "___________________________________________________________________________________________________\n",
      "Example 2\n",
      "___________________________________________________________________________________________________\n",
      "BASELINE HUMAN SUMMARY: \n",
      " #Person2# finds that #Person1# has a fever and says #Person1# should've called in sick earlier.\n",
      "___________________________________________________________________________________________________\n",
      "MODEL GENERATION WITH FEW SHOTS SHOT :\n",
      "#Person1# has a headache and needs someone to help him.\n"
     ]
    }
   ],
   "source": [
    "example_indice=[34,56,11,2,4]\n",
    "\n",
    "### One Shot - ONE COMPLETE EXAMPLE - \n",
    "for i, index in enumerate( example_indices):\n",
    "    summary = dataset['test'][index]['summary']\n",
    "\n",
    "    prompt= make_prompt(example_indice,index)\n",
    "\n",
    "\n",
    "\n",
    "    inputs= tokenizer(prompt,return_tensors='pt') # it returns a pytorch tensor\n",
    "    output= tokenizer.decode(model.generate(\n",
    "        inputs['input_ids'], # input_ids ==> is a part from the input tensor consist from token vectors \n",
    "        max_new_tokens=50\n",
    "\n",
    "    )[0],\n",
    "    skip_special_tokens=True)\n",
    "    print(dash_line)\n",
    "    print(\"Example\",i+1)\n",
    "    print(dash_line)\n",
    "    print(f\"BASELINE HUMAN SUMMARY: \\n {summary}\")\n",
    "    print(dash_line)\n",
    "    print(f'MODEL GENERATION WITH FEW SHOTS SHOT :\\n{output}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configration parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___________________________________________________________________________________________________\n",
      "Example 1\n",
      "___________________________________________________________________________________________________\n",
      "BASELINE HUMAN SUMMARY: \n",
      " In order to prevent employees from wasting time on Instant Message programs, #Person1# decides to terminate the use of those programs and asks Ms. Dawson to send out a memo to all employees by the afternoon.\n",
      "___________________________________________________________________________________________________\n",
      "MODEL GENERATION WITH FEW SHOTS SHOT :\n",
      "Ms. Dawson takes a dictation for #Person1# about prohibiting the use of Instant Message programs in the office.\n",
      "___________________________________________________________________________________________________\n",
      "Example 2\n",
      "___________________________________________________________________________________________________\n",
      "BASELINE HUMAN SUMMARY: \n",
      " #Person2# finds that #Person1# has a fever and says #Person1# should've called in sick earlier.\n",
      "___________________________________________________________________________________________________\n",
      "MODEL GENERATION WITH FEW SHOTS SHOT :\n",
      "#Person1# has a headache and needs someone to help him.\n"
     ]
    }
   ],
   "source": [
    "generation_config= GenerationConfig(max_new_tokens= 50,do_sample=True,top_k =3,top_p=.4,temperature=2.00)\n",
    "\n",
    "#generation_config= GenerationConfig(max_new_tokens= 50,do_sample=True,top_k =6,top_p=.3,temperature=.1)\n",
    "#generation_config= GenerationConfig(max_new_tokens= 50,do_sample=True,top_k =3,top_p=.5,temperature=1)\n",
    "#generation_config= GenerationConfig(max_new_tokens= 50,do_sample=True,top_k =3,top_p=.4,temperature=.5)\n",
    "\n",
    "\n",
    "example_indice=[34,56,11,2,4]\n",
    "\n",
    "for i, index in enumerate( example_indices):\n",
    "    summary = dataset['test'][index]['summary']\n",
    "\n",
    "    prompt= make_prompt(example_indice,index)\n",
    "\n",
    "\n",
    "\n",
    "    inputs= tokenizer(prompt,return_tensors='pt') # it returns a pytorch tensor\n",
    "    output= tokenizer.decode(model.generate(\n",
    "        inputs['input_ids'], # input_ids ==> is a part from the input tensor consist from token vectors \n",
    "        generation_config=generation_config\n",
    "\n",
    "    )[0],\n",
    "    skip_special_tokens=True)\n",
    "    print(dash_line)\n",
    "    print(\"Example\",i+1)\n",
    "    print(dash_line)\n",
    "    print(f\"BASELINE HUMAN SUMMARY: \\n {summary}\")\n",
    "    print(dash_line)\n",
    "    print(f'MODEL GENERATION WITH FEW SHOTS SHOT :\\n{output}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
